{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'vote_average', 'vote_count', 'status', 'release_date',\n",
       "       'revenue', 'runtime', 'adult', 'backdrop_path', 'budget', 'homepage',\n",
       "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
       "       'popularity', 'poster_path', 'tagline', 'genres',\n",
       "       'production_companies', 'production_countries', 'spoken_languages',\n",
       "       'keywords', 'cluster', 'clusters_n'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Carregar os dados\n",
    "dados = pd.read_parquet(\"dataset_com_clusters.parquet\")\n",
    "dados.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classificacao\n",
      "1    0.961524\n",
      "0    0.038476\n",
      "Name: proportion, dtype: float64\n",
      "classificacao\n",
      "1    5248\n",
      "0     210\n",
      "Name: count, dtype: int64\n",
      "classificacao\n",
      "1    0.961517\n",
      "0    0.038483\n",
      "Name: proportion, dtype: float64\n",
      "classificacao\n",
      "1    5247\n",
      "0     210\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def processar_dados(caminho_arquivo):\n",
    "    # Carregar os dados\n",
    "    dados = pd.read_parquet(caminho_arquivo)\n",
    "\n",
    "    # Selecionar colunas relevantes\n",
    "    dados = dados[[\"release_date\", \"revenue\", \"budget\", \"runtime\", \"genres\", \n",
    "                  \"original_language\", \"production_countries\", \"spoken_languages\", \"adult\", \"cluster\", \"clusters_n\"]]\n",
    "\n",
    "    # Filtrar dados (adult == False)\n",
    "    dados = dados[dados[\"adult\"] == False].copy()\n",
    "\n",
    "    # Criar coluna 'disponibilidade_lucro'\n",
    "    dados[\"disponibilidade_lucro\"] = (\n",
    "        dados[\"revenue\"].apply(lambda x: isinstance(x, (int, float)) and x > 0) & \n",
    "        dados[\"budget\"].apply(lambda x: isinstance(x, (int, float)) and x > 0)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Eliminar registros com 'disponibilidade_lucro' igual a 0\n",
    "    dados = dados[dados[\"disponibilidade_lucro\"] == 1].copy()\n",
    "\n",
    "    # Calcular 'lucro' e 'classificacao'\n",
    "    dados['lucro'] = dados.apply(lambda row: row['revenue'] - row['budget']\n",
    "                                 if isinstance(row['budget'], (int, float)) and isinstance(row['revenue'], (int, float))\n",
    "                                 else None, axis=1)\n",
    "\n",
    "    dados['classificacao'] = dados.apply(lambda row: 1 if row['lucro'] is not None and row['lucro'] and row['disponibilidade_lucro'] == 1 else 0, axis=1)\n",
    "\n",
    "    # Criar o dataset 'dados_com_lucro' (classificacao == 1)\n",
    "    dados_com_lucro = dados[dados['classificacao'] == 1]\n",
    "\n",
    "    # Criar o dataset 'dados_sem_lucro' (classificacao == 0)\n",
    "    dados_sem_lucro = dados[dados['classificacao'] == 0]\n",
    "\n",
    "    # Criar duas amostras representativas de 'dados_com_lucro' (50% cada)\n",
    "    metade_lucro_1 = dados_com_lucro.sample(frac=0.5, random_state=42)\n",
    "    metade_lucro_2 = dados_com_lucro.drop(metade_lucro_1.index)  # O restante dos dados\n",
    "\n",
    "    # Criar duas amostras representativas de 'dados_sem_lucro' (50% cada)\n",
    "    metade_sem_lucro_1 = dados_sem_lucro.sample(frac=0.5, random_state=42)\n",
    "    metade_sem_lucro_2 = dados_sem_lucro.drop(metade_sem_lucro_1.index)  # O restante dos dados\n",
    "\n",
    "    # Concatenar as metades sem lucro com suas respectivas metades com lucro\n",
    "    conjunto_1 = pd.concat([metade_lucro_1, metade_sem_lucro_1], ignore_index=True)\n",
    "    conjunto_2 = pd.concat([metade_lucro_2, metade_sem_lucro_2], ignore_index=True)\n",
    "    \n",
    "    return conjunto_1, conjunto_2\n",
    "\n",
    "# Exemplo de uso\n",
    "dados_teste_balanceado, dados_treino_balanceado = processar_dados(\"dataset_com_clusters.parquet\")\n",
    "\n",
    "# Verificar o balanceamento\n",
    "print(dados_teste_balanceado['classificacao'].value_counts(normalize=True))\n",
    "print(dados_teste_balanceado['classificacao'].value_counts())\n",
    "\n",
    "# Verificar o balanceamento\n",
    "print(dados_treino_balanceado['classificacao'].value_counts(normalize=True))\n",
    "print(dados_treino_balanceado['classificacao'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dados_teste = dados_teste_balanceado.copy()\n",
    "\n",
    "dados_treino_filtrado = dados_treino_balanceado.copy()\n",
    "# Criar novas colunas (número de idiomas e idade)\n",
    "for df in [dados_treino_filtrado, dados_teste]:\n",
    "    df[\"num_languages\"] = df[\"spoken_languages\"].apply(lambda x: len(x.split(\",\")) if isinstance(x, str) else 0)\n",
    "    df[\"idade\"] = df[\"release_date\"].apply(lambda x: 2025 - int(x.split(\"-\")[0]) if isinstance(x, str) else 0)\n",
    "\n",
    "# Lista de gêneros\n",
    "generos = ['Action', 'Science Fiction', 'Adventure', 'Drama', 'Crime',\n",
    "           'Thriller', 'Fantasy', 'Comedy', 'Romance', 'Western', 'Mystery', 'War',\n",
    "           'Animation', 'Family', 'Horror', 'Music']\n",
    "\n",
    "def aplicar_one_hot_encoding_generos(df, generos):\n",
    "    for genero in generos:\n",
    "        df[genero] = df['genres'].apply(lambda x: 1 if isinstance(x, str) and genero in x.split(', ') else 0)\n",
    "    return df\n",
    "\n",
    "# Aplicar one-hot encoding de gêneros\n",
    "dados_treino_filtrado = aplicar_one_hot_encoding_generos(dados_treino_filtrado, generos)\n",
    "dados_teste = aplicar_one_hot_encoding_generos(dados_teste, generos)\n",
    "\n",
    "def aplicar_one_hot_encoding_limitado(df, idiomas_permitidos, selected_countries):\n",
    "    df['original_language_encoded'] = df['original_language']\n",
    "    df['production_countries_encoded'] = df['production_countries']\n",
    "    df['original_language_encoded'] = df['original_language_encoded'].apply(\n",
    "        lambda x: x if x in idiomas_permitidos else 'other_language'\n",
    "    )\n",
    "    df['production_countries_encoded'] = df['production_countries_encoded'].apply(\n",
    "        lambda x: x if x in selected_countries else 'other_country'\n",
    "    )\n",
    "    df = pd.get_dummies(df, columns=['original_language_encoded', 'production_countries_encoded'], prefix=['lang', 'country'])\n",
    "    return df\n",
    "\n",
    "# Listas de idiomas e países permitidos\n",
    "idiomas_permitidos = ['en', 'fr', 'es', 'de', 'ja', 'zh', \"pt\", 'it']\n",
    "selected_countries = ['United States', 'France', 'United Kingdom', 'Germany', \n",
    "                      'Canada', 'Japan', 'China', 'India', 'Italy', 'Spain']\n",
    "\n",
    "# Aplicar one-hot encoding limitado\n",
    "dados_treino_encoded = aplicar_one_hot_encoding_limitado(dados_treino_filtrado, idiomas_permitidos, selected_countries)\n",
    "dados_teste_encoded = aplicar_one_hot_encoding_limitado(dados_teste, idiomas_permitidos, selected_countries)\n",
    "\n",
    "# Garantir que a coluna 'genres' seja preservada\n",
    "dados_treino_encoded['genres'] = dados_treino_filtrado['genres']\n",
    "dados_teste_encoded['genres'] = dados_teste['genres']\n",
    "\n",
    "# Alinhar colunas\n",
    "dados_treino_encoded, dados_teste_encoded = dados_treino_encoded.align(dados_teste_encoded, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "dados_treino_encoded.to_parquet(\"dados_treino.parquet\", index=False)\n",
    "dados_teste_encoded.to_parquet(\"dados_teste.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de colunas a serem removidas\n",
    "colunas_para_remover = [\"release_date\", \"revenue\", \"budget\", \"genres\", \"original_language\", \n",
    "                        \"production_countries\", \"spoken_languages\", \"adult\", \n",
    "                        \"disponibilidade_lucro\", \"lucro\", \"runtime\", \"idade\"]\n",
    "\n",
    "# Aplicar a remoção corretamente\n",
    "for i, df in enumerate([dados_teste_encoded, dados_treino_encoded]):\n",
    "    df = df.drop(columns=colunas_para_remover, errors='ignore')\n",
    "\n",
    "    # Atribuir de volta ao DataFrame correto\n",
    "    if i == 0:\n",
    "        dados_teste_encoded = df\n",
    "    else:\n",
    "        dados_treino_encoded = df\n",
    "\n",
    "\n",
    "dados_teste_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Separando features (X) e target (y)\n",
    "X_train = dados_treino_encoded.drop(columns=['classificacao'])\n",
    "y_train = dados_treino_encoded['classificacao']\n",
    "\n",
    "X_test = dados_teste_encoded.drop(columns=['classificacao'])\n",
    "y_test = dados_teste_encoded['classificacao']\n",
    "\n",
    "# Criando e treinando o modelo SVM\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')  # Kernel RBF padrão\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Printando o classification report\n",
    "print(\"Classification Report para Treino (SVM):\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nClassification Report para Teste (SVM):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor valor de k encontrado: 3\n",
      "Melhor recall encontrado: 0.9984756097560976\n",
      "\n",
      "Classification Report para Treino (KNN com melhor k):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.20      0.32       210\n",
      "           1       0.97      1.00      0.98      5247\n",
      "\n",
      "    accuracy                           0.97      5457\n",
      "   macro avg       0.93      0.60      0.65      5457\n",
      "weighted avg       0.97      0.97      0.96      5457\n",
      "\n",
      "\n",
      "Classification Report para Teste (KNN com melhor k):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.11      0.20       210\n",
      "           1       0.97      1.00      0.98      5248\n",
      "\n",
      "    accuracy                           0.96      5458\n",
      "   macro avg       0.86      0.56      0.59      5458\n",
      "weighted avg       0.96      0.96      0.95      5458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "# Loop para encontrar o melhor valor de k com base no recall\n",
    "melhor_k = None\n",
    "melhor_recall = 0\n",
    "for k in range(1, 21):  # Testando valores de k de 1 a 20\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_test_pred = knn_model.predict(X_test)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    if recall > melhor_recall:\n",
    "        melhor_recall = recall\n",
    "        melhor_k = k\n",
    "\n",
    "print(f\"Melhor valor de k encontrado: {melhor_k}\")\n",
    "print(f\"Melhor recall encontrado: {melhor_recall}\")\n",
    "\n",
    "# Treinando o modelo KNN com o melhor valor de k\n",
    "knn_model = KNeighborsClassifier(n_neighbors=melhor_k)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões com o melhor modelo\n",
    "y_train_pred = knn_model.predict(X_train)\n",
    "y_test_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Printando o classification report para o melhor modelo\n",
    "print(\"\\nClassification Report para Treino (KNN com melhor k):\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nClassification Report para Teste (KNN com melhor k):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor valor de n_estimators encontrado: 100\n",
      "Melhor recall encontrado: 0.9929496951219512\n",
      "\n",
      "Classification Report para Treino (Random Forest com melhor n_estimators):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.55      0.68       210\n",
      "           1       0.98      1.00      0.99      5247\n",
      "\n",
      "    accuracy                           0.98      5457\n",
      "   macro avg       0.94      0.77      0.84      5457\n",
      "weighted avg       0.98      0.98      0.98      5457\n",
      "\n",
      "\n",
      "Classification Report para Teste (Random Forest com melhor n_estimators):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.20      0.29       210\n",
      "           1       0.97      0.99      0.98      5248\n",
      "\n",
      "    accuracy                           0.96      5458\n",
      "   macro avg       0.75      0.60      0.64      5458\n",
      "weighted avg       0.95      0.96      0.95      5458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "melhor_n_estimators = None\n",
    "melhor_recall = 0\n",
    "for n_estimators in range(10, 110, 10):  # Testando valores de n_estimators de 10 a 100, pulando de 10 em 10\n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    if recall > melhor_recall:\n",
    "        melhor_recall = recall\n",
    "        melhor_n_estimators = n_estimators\n",
    "\n",
    "print(f\"Melhor valor de n_estimators encontrado: {melhor_n_estimators}\")\n",
    "print(f\"Melhor recall encontrado: {melhor_recall}\")\n",
    "\n",
    "# Treinando o modelo Random Forest com o melhor valor de n_estimators\n",
    "rf_model = RandomForestClassifier(n_estimators=melhor_n_estimators, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões com o melhor modelo\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Printando o classification report para o melhor modelo\n",
    "print(\"\\nClassification Report para Treino (Random Forest com melhor n_estimators):\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nClassification Report para Teste (Random Forest com melhor n_estimators):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor valor de n_estimators encontrado: 10\n",
      "Melhor recall encontrado: 0.9982850609756098\n",
      "\n",
      "Classification Report para Treino (XGBoost com melhor n_estimators):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.18      0.30       210\n",
      "           1       0.97      1.00      0.98      5247\n",
      "\n",
      "    accuracy                           0.97      5457\n",
      "   macro avg       0.93      0.59      0.64      5457\n",
      "weighted avg       0.96      0.97      0.96      5457\n",
      "\n",
      "\n",
      "Classification Report para Teste (XGBoost com melhor n_estimators):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.11      0.20       210\n",
      "           1       0.97      1.00      0.98      5248\n",
      "\n",
      "    accuracy                           0.96      5458\n",
      "   macro avg       0.85      0.56      0.59      5458\n",
      "weighted avg       0.96      0.96      0.95      5458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Loop para encontrar o melhor valor de n_estimators com base no recall\n",
    "melhor_n_estimators = None\n",
    "melhor_recall = 0\n",
    "for n_estimators in range(10, 110, 10):  # Testando valores de n_estimators de 10 a 100, pulando de 10 em 10\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_test_pred = xgb_model.predict(X_test)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    if recall > melhor_recall:\n",
    "        melhor_recall = recall\n",
    "        melhor_n_estimators = n_estimators\n",
    "\n",
    "print(f\"Melhor valor de n_estimators encontrado: {melhor_n_estimators}\")\n",
    "print(f\"Melhor recall encontrado: {melhor_recall}\")\n",
    "\n",
    "# Treinando o modelo XGBoost com o melhor valor de n_estimators\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=melhor_n_estimators, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões com o melhor modelo\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Printando o classification report para o melhor modelo\n",
    "print(\"\\nClassification Report para Treino (XGBoost com melhor n_estimators):\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nClassification Report para Teste (XGBoost com melhor n_estimators):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Using 272 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report para Treino (SVM com sample de 1%):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.94      1.00      0.97       256\n",
      "\n",
      "    accuracy                           0.94       272\n",
      "   macro avg       0.47      0.50      0.48       272\n",
      "weighted avg       0.89      0.94      0.91       272\n",
      "\n",
      "\n",
      "Classification Report para Teste (SVM com sample de 1%):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       210\n",
      "           1       0.96      1.00      0.98      5248\n",
      "\n",
      "    accuracy                           0.96      5458\n",
      "   macro avg       0.48      0.50      0.49      5458\n",
      "weighted avg       0.92      0.96      0.94      5458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/272 [00:14<13:07,  2.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Aplicando o SHAP para explicar o modelo com o sample\u001b[39;00m\n\u001b[1;32m     33\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(svm_model\u001b[38;5;241m.\u001b[39mpredict_proba, X_train_sample)\n\u001b[0;32m---> 34\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Selecionando os SHAP values para a classe positiva (classe 1)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m shap_values_class_1 \u001b[38;5;241m=\u001b[39m shap_values[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# A classe 1 é geralmente a classe positiva\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/repositorios_dsi_pisi/projeto-pisi3/.venv/lib/python3.8/site-packages/shap/explainers/_kernel.py:244\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[1;32m    243\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[0;32m--> 244\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    246\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/Documentos/repositorios_dsi_pisi/projeto-pisi3/.venv/lib/python3.8/site-packages/shap/explainers/_kernel.py:442\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight_left \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[1;32m    445\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n",
      "File \u001b[0;32m~/Documentos/repositorios_dsi_pisi/projeto-pisi3/.venv/lib/python3.8/site-packages/shap/explainers/_kernel.py:581\u001b[0m, in \u001b[0;36mKernelExplainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered:\n\u001b[1;32m    580\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[0;32m--> 581\u001b[0m modelOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modelOut, (pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries)):\n\u001b[1;32m    583\u001b[0m     modelOut \u001b[38;5;241m=\u001b[39m modelOut\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/Documentos/repositorios_dsi_pisi/projeto-pisi3/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:861\u001b[0m, in \u001b[0;36mBaseSVC.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;129m@available_if\u001b[39m(_check_proba)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    836\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    The model needs to have probability information computed at training\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;124;03m    datasets.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobA_\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobB_\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    863\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\n\u001b[1;32m    864\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba is not available when fitted with probability=False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         )\n",
      "File \u001b[0;32m~/Documentos/repositorios_dsi_pisi/projeto-pisi3/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:611\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    608\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[0;32m--> 611\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    621\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m~/Documentos/repositorios_dsi_pisi/projeto-pisi3/.venv/lib/python3.8/site-packages/sklearn/base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Documentos/repositorios_dsi_pisi/projeto-pisi3/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/repositorios_dsi_pisi/projeto-pisi3/.venv/lib/python3.8/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separando features (X) e target (y)\n",
    "X_train = dados_treino_encoded.drop(columns=['classificacao'])\n",
    "y_train = dados_treino_encoded['classificacao']\n",
    "\n",
    "X_test = dados_teste_encoded.drop(columns=['classificacao'])\n",
    "y_test = dados_teste_encoded['classificacao']\n",
    "\n",
    "# Criando um sample representativo de 1% dos dados de treino\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.05, random_state=42)\n",
    "\n",
    "# Criando e treinando o modelo SVM com o sample\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
    "svm_model.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Fazendo previsões com o sample\n",
    "y_train_pred = svm_model.predict(X_train_sample)\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Printando o classification report\n",
    "print(\"Classification Report para Treino (SVM com sample de 1%):\")\n",
    "print(classification_report(y_train_sample, y_train_pred))\n",
    "\n",
    "print(\"\\nClassification Report para Teste (SVM com sample de 1%):\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Aplicando o SHAP para explicar o modelo com o sample\n",
    "explainer = shap.KernelExplainer(svm_model.predict_proba, X_train_sample)\n",
    "shap_values = explainer.shap_values(X_train_sample)\n",
    "\n",
    "# Selecionando os SHAP values para a classe positiva (classe 1)\n",
    "shap_values_class_1 = shap_values[1]  # A classe 1 é geralmente a classe positiva\n",
    "\n",
    "# Visualizando as importâncias das features com beeswarm plot\n",
    "shap.summary_plot(shap_values_class_1, X_train_sample, feature_names=X_train_sample.columns, plot_type=\"dot\")  # plot_type=\"dot\" para beeswarm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
